import argparse
import hashlib
import json
import logging
import os
import shutil
import time
from fnmatch import fnmatch
from glob import glob
from pathlib import Path
from typing import Any, Dict, List, Optional
from tempfile import NamedTemporaryFile
import re

import pandas as pd

BASE = r"C:\Users\ElmarthyJanettyGalla\OneDrive - Weingart Center Association\QA Data Hub"

CFG: Dict[str, Any] = {
    "paths": {
        "staging2": fr"{BASE}\Report Outputs\Second Staging Area",
        "output":   fr"{BASE}\Report Outputs\Output",
        "state_dir":fr"{BASE}\_pipeline_state",
    },
    "reader_defaults": {
        "encoding": "utf-8",
        "csv":   {"header": 0, "na_values": ["", "NA", "N/A", "null", "None"]},
        "excel": {"sheet": "first", "header_row": 0},
    },
    "datasets": [
        {
            "name": "program_client_data",
            "staging1": fr"{BASE}\Staging Area\Biweekly Report\Program Client Data",
            "include_patterns": ["*.csv", "*.xlsx"],
            "exclude_patterns": ["~$*", "*.tmp"],
            "archive_after_process": False,
            "combined_filename": "program_client_data.parquet",
            "renames": {
                "Clients Unique Identifier": "ClientID",
                "Enrollments Active in Project": "ActiveInProject",
                "Clients Client Full Name": "ClientFullName",
                "Clients Active ROI?": "ActiveROI",
                "Enrollments Days in Project": "DaysInProject",
                "Enrollments Project Start Date": "ProjectStartDate",
                "Client Custom Point of Contact Name": "POCName",
                "Client Custom Point of Contact Phone": "POCPhone",
                "Client Custom Point of Contact Email": "POCEmail",
                "Client Custom Point of Contact Date": "POCDate",
                "Clients DoB Data Quality": "DOBDataQuality",
                "Clients SSN Data Quality": "SSNDataQuality",
                "Clients SSN - Last 4": "SSN_Last4",
                "Enrollments Deleted (Yes / No)": "EnrollmentDeleted",
                "Client Assessment Custom TB Clearance Date": "TB_ClearanceDate",
                "Programs Full Name": "ProgramName",
                "Client Assessments Last Assessment ID": "LastAssessmentID",
                "Client Assessments Last Assessment Date": "LastAssessmentDate",
                "List of Client File Name": "ClientFileList",
                "List of Assigned Staff": "AssignedStaff",
            },
            "date_rules": [
                {"column": "ProjectStartDate",  "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                {"column": "POCDate",           "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                {"column": "LastAssessmentDate","formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                {"column": "TB_ClearanceDate",  "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
            ],
            "dtypes": {
                "ClientID": "string",
                "ClientFullName": "string",
                "ActiveROI": "string",
                "ActiveInProject": "string",
                "DaysInProject": "Int64",
                "POCName": "string",
                "POCPhone": "string",
                "POCEmail": "string",
                "DOBDataQuality": "string",
                "SSNDataQuality": "string",
                "SSN_Last4": "string",
                "EnrollmentDeleted": "string",
                "ProgramName": "string",
                "LastAssessmentID": "string",
                "ClientFileList": "string",
                "AssignedStaff": "string",
            },
            "lowercase_columns": ["POCEmail"],
            "drop_queries": ["ClientID.isnull()"],
            "drop_null_any": [],
            "dedup_keys": ["ClientID", "ProgramName"],
            "recency_cols": ["POCDate", "LastAssessmentDate", "ProjectStartDate"],
        },
        {
            "name": "ces_assessments",
            "staging1": fr"{BASE}\Staging Area\Biweekly Report\CES Assessments",
            "include_patterns": ["*.csv", "*.xlsx"],
            "exclude_patterns": ["~$*", "*.tmp"],
            "archive_after_process": False,
            "combined_filename": "ces_assessments.parquet",
            "renames": {
                "Clients Unique Identifier": "ClientID",
                "Programs Full Name": "ProgramName",
                "Client Assessments Assessment ID": "CES_AssessmentID",
                "Client Assessments Assessment Date": "CES_AssessmentDate",
                "Client Assessments Assessment Score": "CES_AssessmentScore",
                "Client Assessments Is Coordinated Entry": "CES_IsCoordinatedEntry",
            },
            "date_rules": [
                {"column": "CES_AssessmentDate", "formats": ["%Y-%m-%d", "%m/%d/%Y"]},
            ],
            "dtypes": {
                "ClientID": "string",
                "ProgramName": "string",
                "CES_AssessmentID": "string",
                "CES_AssessmentScore": "Int64",
                "CES_IsCoordinatedEntry": "string",
            },
            "lowercase_columns": [],
            "drop_queries": ["ClientID.isnull()"],
            "drop_null_any": [],
            "dedup_keys": ["ClientID", "ProgramName", "CES_AssessmentDate", "CES_AssessmentID"],
            "recency_cols": ["CES_AssessmentDate"],
        },
        {
            "name": "case_notes",
            "staging1": fr"{BASE}\Staging Area\Biweekly Report\Case Notes",
            "include_patterns": ["*.csv", "*.xlsx"],
            "exclude_patterns": ["~$*", "*.tmp"],
            "archive_after_process": False,
            "combined_filename": "case_notes.parquet",
            "renames": {},
            "date_rules": [
                {"column": "Month", "formats": ["%Y-%m", "%Y-%m-%d"]},
            ],
            "dtypes": {
                "ClientID": "string",
                "ProgramName": "string",
                "CaseNoteCount": "Int64",
            },
            "lowercase_columns": [],
            "drop_queries": ["ClientID.isnull()"],
            "drop_null_any": [],
            "dedup_keys": ["ClientID", "ProgramName", "Month"],
            "recency_cols": [],
        },
        {
            "name": "services",
            "staging1": fr"{BASE}\Staging Area\Biweekly Report\Services",
            "include_patterns": ["*.csv", "*.xlsx"],
            "exclude_patterns": ["~$*", "*.tmp"],
            "archive_after_process": False,
            "combined_filename": "services.parquet",
            "renames": {
                "Clients Unique Identifier": "ClientID",
                "Programs Full Name": "ProgramName",
                "Services Start Date Month": "ServicesMonth",
                "Services Count": "ServicesCount",
            },
            "date_rules": [
                {"column": "ServicesMonth", "formats": ["%Y-%m", "%m/%d/%Y"]},
            ],
            "dtypes": {
                "ClientID": "string",
                "ProgramName": "string",
                "ServicesCount": "Int64",
            },
            "lowercase_columns": [],
            "drop_queries": ["ClientID.isnull()"],
            "drop_null_any": [],
            "dedup_keys": ["ClientID", "ProgramName", "ServicesMonth"],
            "recency_cols": [],
        },
    ],
    "output": {
        "final_view_basename": "WCA_Biweekly_Final",
        "write_csv": True,
        "write_parquet": True,
        "write_excel": True,
        "index": False
    },
    "logging": {"level": "INFO"}
}

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def list_ingest_files(folder: Path, include: List[str], exclude: List[str]) -> List[Path]:
    hits: List[Path] = []
    for pat in include:
        hits.extend([Path(p) for p in glob(str(folder / pat))])
    files: List[Path] = []
    for f in hits:
        if any(fnmatch(f.name, ex) for ex in exclude):
            continue
        if f.is_file():
            files.append(f)
    return sorted(files)

def md5_bytes(b: bytes) -> str:
    m = hashlib.md5()
    m.update(b)
    return m.hexdigest()

def read_bytes(p: Path) -> bytes:
    with open(p, "rb") as f:
        return f.read()

def parse_date_from_name(name: str) -> Optional[pd.Timestamp]:
    patterns = [r"(\d{1,2})[-_](\d{1,2})[-_](\d{4})"]
    for pat in patterns:
        m = re.findall(pat, name)
        if m:
            mm, dd, yyyy = m[-1]
            try:
                return pd.Timestamp(year=int(yyyy), month=int(mm), day=int(dd))
            except Exception:
                continue
    return None

def pick_latest_file(files: List[Path]) -> Optional[Path]:
    if not files:
        return None
    def key(fp: Path):
        d = parse_date_from_name(fp.stem)
        if d is not None:
            return d
        try:
            return pd.Timestamp.fromtimestamp(fp.stat().st_mtime)
        except Exception:
            return pd.Timestamp(1970,1,1)
    return sorted(files, key=key)[-1]

def read_csv(fp: Path, defaults: Dict[str, Any]) -> pd.DataFrame:
    encoding = defaults.get("encoding", "utf-8")
    csv_cfg = defaults.get("csv", {})
    try:
        return pd.read_csv(fp, encoding=encoding, **csv_cfg)
    except UnicodeDecodeError:
        return pd.read_csv(fp, encoding="cp1252", **csv_cfg)

def read_excel(fp: Path, defaults: Dict[str, Any]) -> pd.DataFrame:
    xl = defaults.get("excel", {})
    sheet = xl.get("sheet", "first")
    header = xl.get("header_row", 0)
    if isinstance(sheet, str) and sheet.lower() == "first":
        sheet = 0
    return pd.read_excel(fp, sheet_name=sheet, header=header)

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [c.strip() for c in df.columns]
    return df

def ensure_unique_columns(df: pd.DataFrame, dataset_name: str = "") -> pd.DataFrame:
    dup_mask = df.columns.duplicated(keep="first")
    if dup_mask.any():
        df = df.loc[:, ~dup_mask].copy()
    return df

def apply_column_renames(df: pd.DataFrame, renames: Dict[str, str]) -> pd.DataFrame:
    if not renames:
        return df
    exist = {k: v for k, v in renames.items() if k in df.columns}
    return df.rename(columns=exist)

def parse_dates(df: pd.DataFrame, date_rules: List[Dict[str, Any]]) -> pd.DataFrame:
    for r in date_rules:
        col = r["column"]
        if col not in df.columns:
            continue
        series = df[col]
        fmts = r.get("formats")
        if fmts:
            out = pd.to_datetime(pd.Series([pd.NA] * len(df), index=df.index), errors="coerce")
            for fmt in fmts:
                attempt = pd.to_datetime(series, errors="coerce", format=fmt)
                out = out.fillna(attempt)
            df[col] = out
        else:
            df[col] = pd.to_datetime(series, errors="coerce")
    return df

def enforce_dtypes(df: pd.DataFrame, dtypes: Dict[str, str]) -> pd.DataFrame:
    for col, dt in dtypes.items():
        if col not in df.columns():
            continue
        try:
            if dt == "string":
                df[col] = df[col].astype("string")
            else:
                df[col] = df[col].astype(dt, errors="ignore")
        except Exception:
            pass
    return df

def enforce_dtypes(df: pd.DataFrame, dtypes: Dict[str, str]) -> pd.DataFrame:
    for col, dt in dtypes.items():
        if col not in df.columns:
            continue
        try:
            if dt == "string":
                df[col] = df[col].astype("string")
            else:
                df[col] = df[col].astype(dt, errors="ignore")
        except Exception:
            pass
    return df

def trim_strings(df: pd.DataFrame) -> pd.DataFrame:
    obj_cols = df.select_dtypes(include=["object", "string"]).columns
    for c in obj_cols:
        df[c] = df[c].astype("string").str.strip()
    return df

def to_lower(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    for c in cols:
        if c in df.columns:
            df[c] = df[c].astype("string").str.lower()
    return df

def drop_bad_rows(df: pd.DataFrame, queries: List[str], null_any: List[str]) -> pd.DataFrame:
    mask = pd.Series(True, index=df.index)
    for q in queries:
        try:
            bad_idx = df.query(q, engine="python").index
            mask.loc[bad_idx] = False
        except Exception:
            continue
    for col in null_any:
        if col in df.columns:
            mask &= df[col].notna()
    return df.loc[mask]

def deduplicate(df: pd.DataFrame, keys: List[str], recency_cols: List[str]) -> pd.DataFrame:
    sort_cols = [c for c in recency_cols if c in df.columns]
    if sort_cols:
        df = df.sort_values(sort_cols)
    if keys:
        keep_cols = [k for k in keys if k in df.columns]
        if keep_cols:
            df = df.drop_duplicates(subset=keep_cols, keep="last")
    else:
        df = df.drop_duplicates(keep="last")
    return df.reset_index(drop=True)

def preprocess_case_notes_to_long(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["ClientID","ProgramName","Month","CaseNoteCount"])
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    if {"ClientID","ProgramName","Month","CaseNoteCount"}.issubset(df.columns):
        return df[["ClientID","ProgramName","Month","CaseNoteCount"]].copy()
    if "Clients Unique Identifier" not in df.columns and len(df) >= 2:
        new_cols = df.iloc[1].tolist()
        df = df.iloc[2:].copy()
        df.columns = [str(c).strip() for c in new_cols]
    df.columns = [str(c).strip() for c in df.columns]
    lower_map = {c.lower(): c for c in df.columns}
    def pick(possibles):
        for p in possibles:
            if p.lower() in lower_map:
                return lower_map[p.lower()]
        return None
    id_col  = pick(["ClientID","Clients Unique Identifier"])
    prg_col = pick(["ProgramName","Programs Full Name"])
    if id_col is None or prg_col is None:
        return pd.DataFrame(columns=["ClientID","ProgramName","Month","CaseNoteCount"])
    df = df.rename(columns={id_col: "ClientID", prg_col: "ProgramName"})
    month_cols = [c for c in df.columns if isinstance(c,str) and len(c)==7 and c[4]=="-" and c[:4].isdigit() and c[5:].isdigit()]
    if month_cols:
        tmp = df[["ClientID","ProgramName"] + month_cols].copy()
        long_df = tmp.melt(id_vars=["ClientID","ProgramName"], value_vars=month_cols,
                           var_name="Month", value_name="CaseNoteCount")
        long_df["Month"] = pd.to_datetime(long_df["Month"], format="%Y-%m", errors="coerce")
        long_df["CaseNoteCount"] = pd.to_numeric(long_df["CaseNoteCount"], errors="coerce")
        long_df = long_df.dropna(subset=["ClientID"])
        return long_df[["ClientID","ProgramName","Month","CaseNoteCount"]]
    count_like = [c for c in df.columns if str(c).startswith("Client Notes - Enrollment Level Count")]
    if count_like:
        today = pd.Timestamp.today().to_period("M")
        periods = [today - i for i in range(len(count_like), 0, -1)]
        long_rows = []
        for i, col in enumerate(count_like):
            period = periods[i] if i < len(periods) else today
            tmp = df[["ClientID","ProgramName", col]].copy()
            tmp["Month"] = period.to_timestamp()
            tmp = tmp.rename(columns={col: "CaseNoteCount"})
            long_rows.append(tmp)
        out = pd.concat(long_rows, ignore_index=True)
        out["CaseNoteCount"] = pd.to_numeric(out["CaseNoteCount"], errors="coerce")
        out = out.dropna(subset=["ClientID"])
        return out[["ClientID","ProgramName","Month","CaseNoteCount"]]
    return pd.DataFrame(columns=["ClientID","ProgramName","Month","CaseNoteCount"])

def services_wide_all_months(services: pd.DataFrame) -> pd.DataFrame:
    if services.empty or "ServicesMonth" not in services.columns:
        return pd.DataFrame()
    svc = services.copy()
    svc["Period"] = svc["ServicesMonth"].dt.to_period("M")
    svc = svc.groupby(["ClientID","ProgramName","Period"], as_index=False)["ServicesCount"].sum()
    svc["Col"] = svc["Period"].apply(lambda p: f"{p.strftime('%B %Y')} Service Notes Count")
    wide = svc.pivot_table(index=["ClientID","ProgramName"], columns="Col", values="ServicesCount", aggfunc="sum").reset_index()
    wide.columns.name = None
    return wide

def case_notes_wide_all_months(notes_long: pd.DataFrame) -> pd.DataFrame:
    if notes_long.empty or "Month" not in notes_long.columns:
        return pd.DataFrame()
    cn = notes_long.copy()
    cn["Period"] = cn["Month"].dt.to_period("M")
    cn = cn.groupby(["ClientID","ProgramName","Period"], as_index=False)["CaseNoteCount"].sum()
    cn["Col"] = cn["Period"].apply(lambda p: f"{p.strftime('%B %Y')} Case Notes")
    wide = cn.pivot_table(index=["ClientID","ProgramName"], columns="Col", values="CaseNoteCount", aggfunc="sum").reset_index()
    wide.columns.name = None
    return wide

def ces_latest_scored(ces: pd.DataFrame) -> pd.DataFrame:
    cols = ["ClientID", "Latest CES Assessment Date", "CES Assessment Score"]
    if ces is None or ces.empty:
        return pd.DataFrame(columns=cols)
    r = ces.copy()
    r["ClientID"] = r["ClientID"].astype("string")
    r["CES_AssessmentDate"] = pd.to_datetime(r["CES_AssessmentDate"], errors="coerce").dt.normalize()
    r["CES_AssessmentScore"] = pd.to_numeric(r["CES_AssessmentScore"], errors="coerce")
    r = r.dropna(subset=["ClientID","CES_AssessmentDate","CES_AssessmentScore"])
    if r.empty:
        return pd.DataFrame(columns=cols)
    r = r.sort_values(["ClientID","CES_AssessmentDate"])
    latest = (
        r.drop_duplicates(subset=["ClientID"], keep="last")[["ClientID","CES_AssessmentDate","CES_AssessmentScore"]]
        .rename(columns={"CES_AssessmentDate":"Latest CES Assessment Date","CES_AssessmentScore":"CES Assessment Score"})
    )
    latest["CES Assessment Score"] = latest["CES Assessment Score"].round().astype("Int64")
    return latest.reset_index(drop=True)

def flags_from_filelist(df: pd.DataFrame) -> pd.DataFrame:
    txt = df.get("ClientFileList", pd.Series(index=df.index, dtype="string")).astype("string").fillna("")
    txt_norm = txt.str.replace("’", "'", regex=False)
    def has(substr: str, s=txt_norm):
        return s.str.contains(substr, case=False, na=False, regex=False)
    income_parts = [
        ("Pay Stub", "Pay Stub"),
        ("SSDI", "Supplemental Security Disability Income (SSDI) Forms"),
        ("SSI", "Supplemental Security Income (SSI) Forms"),
        ("GR", "General Relief (GR) Form"),
        ("Food Stamp", "Food Stamp Card or Award Letter"),
        ("CalWORKS", "CalWORKS Forms"),
        ("Form 1087", "Form 1087 - Self Declaration of Income/No Income Form"),
        ("Form 1084", "Form 1084 - 3rd Party Income Verification"),
        ("Alimony Agreement", "Alimony Agreement"),
        ("Social Security (NUMI) Printout", "Social Security (NUMI) Printout"),
        ("Tax Return", "Tax Return"),
        ("Veterans Affairs (VA) Benefits Award Letter", "Veterans Affairs (VA) Benefits Award Letter"),
        ("Self Employment Document", "Self Employment Document"),
        ("Other Financial Document", "Other Financial Document"),
    ]
    poilist = pd.Series("", index=df.index, dtype="string")
    for label, needle in income_parts:
        mask = has(needle)
        poilist = poilist.mask(mask & (poilist == ""), label)
        poilist = poilist.mask(mask & (poilist != ""), poilist + ", " + label)
    df["Proof of Income"] = poilist.mask(poilist == "", "0").where(~(poilist == ""), "1 - " + poilist)
    hilist = pd.Series("", index=df.index, dtype="string")
    for label, needle in [("Medicare/Medicaid", "Medicaid or Medicare Card"),
                          ("Other Health Insurance", "Health Insurance Documentation")]:
        mask = has(needle)
        hilist = hilist.mask(mask & (hilist == ""), label)
        hilist = hilist.mask(mask & (hilist != ""), hilist + ", " + label)
    df["Health Insurance"] = hilist.mask(hilist == "", "0").where(~(hilist == ""), "1 - " + hilist)
    df["SSN Card"] = has("Social Security Card").astype(int)
    df["Birth Certificate"] = has("Birth Certificate or Hospital Record of Birth").astype(int)
    df["CDL or State ID"] = txt_norm.str.contains("Driver's License/State ID Card/Photo ID/ School Identification Card", case=False, na=False, regex=False).astype(int)
    df["Disability Verification"] = has("Disability Verification").astype(int)
    return df

def build_final_view_from_parquets(staging2_dir: Path) -> pd.DataFrame:
    prog_path = staging2_dir / "program_client_data.parquet"
    if not prog_path.exists():
        logging.error("No Program Client Data parquet found. Place the latest file in the Program Client Data folder (or its Archive) and rerun.")
        return pd.DataFrame()
    prog = pd.read_parquet(prog_path)
    ces_path  = staging2_dir / "ces_assessments.parquet"
    notes_path= staging2_dir / "case_notes.parquet"
    serv_path = staging2_dir / "services.parquet"
    ces  = pd.read_parquet(ces_path)   if ces_path.exists()   else pd.DataFrame()
    notes= pd.read_parquet(notes_path) if notes_path.exists() else pd.DataFrame()
    serv = pd.read_parquet(serv_path)  if serv_path.exists()  else pd.DataFrame()
    notes_long = notes.copy()
    if not notes_long.empty and "Month" not in notes_long.columns:
        notes_long = preprocess_case_notes_to_long(notes_long)
    notes_wide = case_notes_wide_all_months(notes_long) if not notes_long.empty else pd.DataFrame()
    serv_wide  = services_wide_all_months(serv)         if not serv.empty      else pd.DataFrame()
    ces_latest = ces_latest_scored(ces)                 if not ces.empty       else pd.DataFrame(columns=["ClientID","Latest CES Assessment Date","CES Assessment Score"])
    df = prog.copy()
    if not notes_wide.empty:
        df = df.merge(notes_wide, how="left", on=["ClientID","ProgramName"])
    if not serv_wide.empty:
        df = df.merge(serv_wide,  how="left", on=["ClientID","ProgramName"])
    if not ces_latest.empty:
        df = df.merge(ces_latest, how="left", on=["ClientID"])
    df = flags_from_filelist(df)
    today = pd.Timestamp.today().normalize()
    def ces_status(dt):
        if pd.isna(dt):
            return "CES Not Done"
        delta = (today - pd.to_datetime(dt)).days
        return "Renewal Overdue" if delta > 730 else "Current"
    df["CES Status"] = df["Latest CES Assessment Date"].apply(ces_status)
    ces_score = pd.to_numeric(df.get("CES Assessment Score"), errors="coerce")
    df["Document Ready"] = (
        (df.get("CDL or State ID", 0).fillna(0).astype(int) == 1) &
        (df.get("SSN Card", 0).fillna(0).astype(int) == 1) &
        df.get("Proof of Income", "").astype("string").str.match(r"^1\b", na=False)
    ).map({True: "Document Ready", False: "Not Document Ready"})
    df["Housing Matching Ready"] = (
        (df.get("CDL or State ID", 0).fillna(0).astype(int) == 1) &
        (df.get("SSN Card", 0).fillna(0).astype(int) == 1) &
        (df.get("Proof of Income", "").astype("string").str.match(r"^1\b", na=False)) &
        (ces_score.fillna(0) >= 8)
    ).map({True: "Housing Matching Ready", False: "Not Housing Matching Ready"})
    days = pd.to_numeric(df.get("DaysInProject"), errors="coerce")
    docs_ready = df["Document Ready"] != "Not Document Ready"
    low_ces = ces_score.isna() | (ces_score < 8)
    ces_expired = df["CES Status"].eq("Renewal Overdue")
    long_stay = days.ge(120)
    mid_stay  = days.ge(75) & days.lt(120)
    alert = pd.Series(pd.NA, index=df.index, dtype="string")
    alert = alert.mask(long_stay & ~docs_ready, "≥120 days & missing docs – ESCALATE")
    alert = alert.mask(long_stay & ces_expired, "≥120 days & CES expired – ESCALATE")
    alert = alert.mask(long_stay & low_ces, "≥120 days & CES<8 – ESCALATE")
    alert = alert.mask(long_stay & ~(~docs_ready | ces_expired | low_ces), "≥120 days & docs/CES OK – monitor")
    alert = alert.mask(mid_stay & ~docs_ready, "75–119 days & missing docs – ACTION")
    alert = alert.mask(mid_stay & (ces_expired | low_ces), "75–119 days & CES risk – ACTION")
    df["Intervention Alert"] = alert
    rename_back = {
        "ClientID": "Clients Unique Identifier",
        "ActiveInProject": "Enrollments Active in Project",
        "ClientFullName": "Clients Client Full Name",
        "ActiveROI": "Clients Active ROI?",
        "DaysInProject": "Enrollments Days in Project",
        "ProjectStartDate": "Enrollments Project Start Date",
        "POCName": "Client Custom Point of Contact Name",
        "POCPhone": "Client Custom Point of Contact Phone",
        "POCEmail": "Client Custom Point of Contact Email",
        "POCDate": "Client Custom Point of Contact Date",
        "DOBDataQuality": "Clients DoB Data Quality",
        "SSNDataQuality": "Clients SSN Data Quality",
        "TB_ClearanceDate": "Client Assessment Custom TB Clearance Date",
        "ProgramName": "Programs Full Name",
        "LastAssessmentID": "Client Assessments Last Assessment ID",
        "LastAssessmentDate": "Client Assessments Last Assessment Date",
        "AssignedStaff": "List of Assigned Staff",
        "ClientFileList": "List of Client File Name",
    }
    df = df.rename(columns=rename_back)
    for c in ["SSN Card","Birth Certificate","CDL or State ID","Disability Verification"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)
    if "Proof of Income" in df.columns:
        df["Proof of Income"] = df["Proof of Income"].astype("string").fillna("0")
    if "Bed Number" not in df.columns:
        df["Bed Number"] = pd.NA
    for col in [
        "Enrollments Project Start Date",
        "Client Custom Point of Contact Date",
        "Client Assessments Last Assessment Date",
        "Latest CES Assessment Date",
        "Client Assessment Custom TB Clearance Date",
    ]:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce").apply(lambda x: f"{x.month}/{x.day}/{x.year}" if not pd.isna(x) else pd.NA)
    front_cols = [
        "Clients Unique Identifier",
        "Enrollments Active in Project",
        "Clients Client Full Name",
        "Clients Active ROI?",
        "Enrollments Days in Project",
        "Enrollments Project Start Date",
        "Client Custom Point of Contact Name",
        "Client Custom Point of Contact Phone",
        "Client Custom Point of Contact Email",
        "Client Custom Point of Contact Date",
        "Clients DoB Data Quality",
        "Clients SSN Data Quality",
        "Client Assessment Custom TB Clearance Date",
        "Programs Full Name",
        "Client Assessments Last Assessment ID",
        "Client Assessments Last Assessment Date",
        "List of Assigned Staff",
        "SSN Card",
        "Birth Certificate",
        "CDL or State ID",
        "Disability Verification",
        "Proof of Income",
        "Bed Number",
    ]
    month_cols = [c for c in df.columns if c.endswith(" Case Notes") or c.endswith(" Service Notes Count")]
    tail_cols = [
        "Latest CES Assessment Date",
        "CES Assessment Score",
        "CES Status",
        "Document Ready",
        "Housing Matching Ready",
        "Intervention Alert",
        "SourceFile",
        "SourceDataset",
    ]
    for c in front_cols + month_cols + tail_cols:
        if c not in df.columns:
            df[c] = pd.NA
    df = df[front_cols + month_cols + tail_cols]
    return df

def process_dataset(
    ds_cfg: Dict[str, Any],
    staging2_dir: Path,
    state_dir: Path,
    reader_defaults: Dict[str, Any],
    dry_run: bool=False
) -> Optional[pd.DataFrame]:
    name = ds_cfg["name"]
    staging1 = Path(ds_cfg["staging1"])
    include = ds_cfg.get("include_patterns", ["*.csv", "*.xlsx"])
    exclude = ds_cfg.get("exclude_patterns", [])
    ensure_dir(staging1)
    ensure_dir(staging2_dir)
    archive_dir = staging1 / "Archive"
    files = list_ingest_files(staging1, include, exclude)
    if archive_dir.exists():
        files += list_ingest_files(archive_dir, include, exclude)
    files = sorted(set(files))
    latest = pick_latest_file(files)
    logging.info(f"[{name}] Found {len(files)} file(s); using latest: {latest.name if latest else 'None'}")
    if latest is None:
        return pd.DataFrame()
    try:
        if latest.suffix.lower() == ".csv":
            df = read_csv(latest, reader_defaults)
        elif latest.suffix.lower() in [".xlsx", ".xls"]:
            df = read_excel(latest, reader_defaults)
        else:
            logging.info(f"[{name}] Unsupported file type: {latest.name}")
            return pd.DataFrame()
        if df is None or df.empty:
            logging.info(f"[{name}] Empty/unreadable: {latest.name}")
            return pd.DataFrame()
        df = normalize_columns(df)
        df = ensure_unique_columns(df, dataset_name=name)
        if name == "case_notes":
            try:
                df = preprocess_case_notes_to_long(df)
            except Exception as e:
                logging.exception(f"[{name}] Preprocess failed for {latest.name}: {e}")
        df["SourceFile"] = latest.name
        df["SourceDataset"] = name
        df = apply_column_renames(df, ds_cfg.get("renames", {}))
        df = parse_dates(df, ds_cfg.get("date_rules", []))
        df = enforce_dtypes(df, ds_cfg.get("dtypes", {}))
        df = trim_strings(df)
        df = to_lower(df, ds_cfg.get("lowercase_columns", []))
        df = drop_bad_rows(df, ds_cfg.get("drop_queries", []), ds_cfg.get("drop_null_any", []))
        df = deduplicate(df, ds_cfg.get("dedup_keys", []), ds_cfg.get("recency_cols", []))
        out_path = staging2_dir / ds_cfg.get("combined_filename", f"{name}.parquet")
        if not dry_run:
            df.to_parquet(out_path, index=False)
            logging.info(f"[{name}] Wrote latest parquet to {out_path} ({len(df):,} rows).")
        else:
            logging.info(f"[{name}] [Dry-run] Would write latest parquet to {out_path}.")
        return df
    except Exception as e:
        logging.exception(f"[{name}] Failed ingest for {latest}: {e}")
        return pd.DataFrame()

def atomic_replace(src: Path, dst: Path, attempts: int = 12, delay: int = 2):
    last_err = None
    for _ in range(attempts):
        try:
            src.replace(dst)
            return
        except PermissionError as e:
            last_err = e
            time.sleep(delay)
    if last_err:
        raise last_err

def atomic_write_parquet(df: pd.DataFrame, path: Path, **kwargs):
    path = Path(path)
    ensure_dir(path.parent)
    with NamedTemporaryFile("wb", delete=False, dir=path.parent, suffix=".tmp") as tf:
        tmp = Path(tf.name)
    df.to_parquet(tmp, **kwargs)
    atomic_replace(tmp, path)

def atomic_write_csv(df: pd.DataFrame, path: Path, **kwargs):
    path = Path(path)
    ensure_dir(path.parent)
    encoding = kwargs.pop("encoding", "utf-8-sig")
    newline = kwargs.pop("newline", "")
    with NamedTemporaryFile("w", delete=False, dir=path.parent, suffix=".tmp", encoding=encoding, newline=newline) as tf:
        tmp = Path(tf.name)
    df.to_csv(tmp, **kwargs)
    atomic_replace(tmp, path)

def write_excel_splitting(df: pd.DataFrame, out_path_base: Path, sheet_name: str = "Sheet1", max_rows: int = 1_000_000):
    ensure_dir(out_path_base.parent)
    n = len(df)
    if n == 0:
        dst = out_path_base.with_suffix(".xlsx")
        with NamedTemporaryFile("wb", delete=False, dir=dst.parent, suffix=".tmp.xlsx") as tf:
            tmp = Path(tf.name)
        with pd.ExcelWriter(tmp, engine="openpyxl") as xw:
            pd.DataFrame().to_excel(xw, index=False, sheet_name=sheet_name)
        atomic_replace(tmp, dst)
        return
    parts = (n + max_rows - 1) // max_rows
    if parts == 1:
        dst = out_path_base.with_suffix(".xlsx")
        with NamedTemporaryFile("wb", delete=False, dir=dst.parent, suffix=".tmp.xlsx") as tf:
            tmp = Path(tf.name)
        with pd.ExcelWriter(tmp, engine="openpyxl") as xw:
            df.to_excel(xw, index=False, sheet_name=sheet_name)
        atomic_replace(tmp, dst)
    else:
        for i in range(parts):
            lo = i * max_rows
            hi = min((i + 1) * max_rows, n)
            part_df = df.iloc[lo:hi].copy()
            dst = out_path_base.with_name(out_path_base.stem + f"_part{i+1}").with_suffix(".xlsx")
            with NamedTemporaryFile("wb", delete=False, dir=dst.parent, suffix=".tmp.xlsx") as tf:
                tmp = Path(tf.name)
            with pd.ExcelWriter(tmp, engine="openpyxl") as xw:
                part_df.to_excel(xw, index=False, sheet_name=sheet_name)
            atomic_replace(tmp, dst)

def run_pipeline(cfg: Dict[str, Any], dry_run: bool=False):
    logging.basicConfig(level=getattr(logging, cfg.get("logging", {}).get("level", "INFO")))
    logging.info("=== Starting HMIS ETL (latest files only; no historical) ===")
    staging2_dir = Path(cfg["paths"]["staging2"])
    output_dir   = Path(cfg["paths"]["output"])
    state_dir    = Path(cfg["paths"]["state_dir"])
    for p in [staging2_dir, output_dir, state_dir]:
        ensure_dir(p)
    reader_defaults = cfg["reader_defaults"]
    datasets = cfg["datasets"]
    for ds in datasets:
        process_dataset(ds, staging2_dir, state_dir, reader_defaults, dry_run=dry_run)
    if dry_run:
        logging.info("[Dry-run] Skipping outputs.")
        logging.info("=== Done (dry-run) ===")
        return
    curated = build_final_view_from_parquets(staging2_dir)
    if curated.empty:
        logging.error("Final view is empty because required latest files were not found. Add the latest HMIS files (CSV/XLSX) to the staging folders (or their Archive subfolders) and rerun.")
        return
    vb = cfg["output"].get("final_view_basename", "WCA_Biweekly_Final")
    if cfg["output"].get("write_parquet", True):
        atomic_write_parquet(curated, output_dir / f"{vb}.parquet", index=cfg["output"].get("index", False))
    if cfg["output"].get("write_csv", True):
        atomic_write_csv(curated, output_dir / f"{vb}.csv", index=cfg["output"].get("index", False))
    if cfg["output"].get("write_excel", True):
        write_excel_splitting(curated, output_dir / vb)
    logging.info(f"[output] Wrote {vb}.* to {output_dir}")
    logging.info("=== Pipeline complete ===")

if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="Biweekly HMIS ETL (latest files only)")
    ap.add_argument("--dry-run", action="store_true")
    args = ap.parse_args()
    run_pipeline(CFG, dry_run=args.dry_run)
